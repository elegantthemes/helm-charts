apiVersion: pxc.percona.com/v1-6-0
kind: PerconaXtraDBCluster
metadata:
  name: {{ .Values.cluster_name }}
  labels:
    {{ .Values.label_name }}: {{ .Values.label_value }}
    release: {{ .Values.label_value }}
  finalizers:
    - delete-pxc-pods-in-order
#   - delete-proxysql-pvc
#   - delete-pxc-pvc
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"pxc.percona.com/v1-6-0","kind":"PerconaXtraDBCluster"}
spec:
  crVersion: 1.6.0
  secretsName: {{ .Values.secrets_name }}
  sslSecretName: {{ .Values.ssl_secrets_name }}
  sslInternalSecretName: {{ .Values.ssl_internal_secrets_name }}
  vaultSecretName: {{ .Values.vault_secrets_name }}
  allowUnsafeConfigurations: false
  #  pause: false
  updateStrategy: SmartUpdate
  upgradeOptions:
    versionServiceEndpoint: https://check.percona.com/versions
    apply: Disabled
    schedule: "0 4 * * *"
  pxc:
    size: 3
    image: percona/percona-xtradb-cluster:8.0.20-11.1
#    readinessDelaySec: 15
#    livenessDelaySec: 300
#    forceUnsafeBootstrap: false
    configuration: |
      [mysqld]
      #gtid_mode = ON
      #enforce_gtid_consistency = ON
      #session_track_gtids = OWN_GTID
      wsrep_sync_wait = 1
      [sst]
      xbstream-opts = --decompress
      [xtrabackup]
      compress = lz4
    resources:
      requests:
        memory: 2G
        cpu: 2000m
      limits:
    #   memory: 1G
    #   cpu: "1"
    # nodeSelector:
    #   disktype: ssd
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
    podDisruptionBudget:
      maxUnavailable: 1
    volumeSpec:
      hostPath:
        path: /data
        type: DirectoryOrCreate
      #persistentVolumeClaim:
      #  storageClassName: {{ .Values.storage_class_name }}
      #  accessModes: [ "ReadWriteOnce" ]
      #  resources:
      #    requests:
      #      storage: 6Gi
    gracePeriod: 600
  haproxy:
    enabled: false
    size: 3
    image: percona/percona-xtradb-cluster-operator:1.6.0-haproxy
#    configuration: |
#      global
#        maxconn 2048
#        external-check
#        stats socket /var/run/haproxy.sock mode 600 expose-fd listeners level user
#
#      defaults
#        log global
#        mode tcp
#        retries 10
#        timeout client 28800s
#        timeout connect 100500
#        timeout server 28800s
#
#      frontend galera-in
#        bind *:3309 accept-proxy
#        bind *:3306 accept-proxy
#        mode tcp
#        option clitcpka
#        default_backend galera-nodes
#
#      frontend galera-replica-in
#        bind *:3307
#        mode tcp
#        option clitcpka
#        default_backend galera-replica-nodes
#    serviceType: ClusterIP
#    externalTrafficPolicy: Cluster
#    replicasServiceType: ClusterIP
#    replicasExternalTrafficPolicy: Cluster
#    schedulerName: "default"
    resources:
      requests:
        memory: 1G
        cpu: 600m
#      limits:
#        memory: 1G
#        cpu: 700m
#    priorityClassName: high-priority
#    nodeSelector:
#      disktype: ssd
#    sidecarResources:
#      requests:
#        memory: 1G
#        cpu: 500m
#      limits:
#        memory: 2G
#        cpu: 600m
#    serviceAccountName: percona-xtradb-cluster-operator-workload
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
#      advanced:
#        nodeAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#            nodeSelectorTerms:
#            - matchExpressions:
#              - key: kubernetes.io/e2e-az-name
#                operator: In
#                values:
#                - e2e-az1
#                - e2e-az2
#    tolerations:
#    - key: "node.alpha.kubernetes.io/unreachable"
#      operator: "Exists"
#      effect: "NoExecute"
#      tolerationSeconds: 6000
    podDisruptionBudget:
      maxUnavailable: 1
#      minAvailable: 0
    gracePeriod: 30
#   loadBalancerSourceRanges:
#     - 10.0.0.0/8
#   serviceAnnotations:
#     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  proxysql:
    enabled: false
    size: 3
    # Applied patch from https://github.com/sysown/proxysql/pull/2956
    image: elegantthemes/percona-xtradb-cluster-operator:1.6.0_3-proxysql
    configuration: |
      datadir="/var/lib/proxysql"
      errorlog="/var/lib/proxysql/proxysql.log"
    
      admin_variables =
      {
        admin_credentials="proxyadmin:{{ .Values.proxysql_admin_password }}"
        mysql_ifaces="0.0.0.0:6032"
        refresh_interval=2000
    
        cluster_check_interval_ms=200
        cluster_check_status_frequency=100
        cluster_mysql_query_rules_diffs_before_sync=3
        cluster_mysql_query_rules_save_to_disk=true
        cluster_mysql_servers_diffs_before_sync=3
        cluster_mysql_servers_save_to_disk=true
        cluster_mysql_users_diffs_before_sync=3
        cluster_mysql_users_save_to_disk=true
        cluster_password="{{ .Values.proxysql_admin_password }}"
        cluster_proxysql_servers_diffs_before_sync=3
        cluster_proxysql_servers_save_to_disk=true
        cluster_username="proxyadmin"
      }
    
      mysql_variables=
      {
        commands_stats=true
        connect_timeout_server=10000
        default_query_delay=0
        default_query_timeout=10000
        default_schema="information_schema"
        have_ssl=true
        interfaces="0.0.0.0:3306;0.0.0.0:33062"
        max_connections=2048
        monitor_connect_interval=20000
        monitor_galera_healthcheck_interval=5000
        monitor_galera_healthcheck_max_timeout_count=5
        monitor_galera_healthcheck_timeout=1200
        monitor_history=60000
        monitor_password="{{ .Values.proxysql_monitor_password }}"
        monitor_ping_interval=10000
        ping_timeout_server=200
        poll_timeout=2000
        sessions_sort=true
        ssl_p2s_ca="/etc/proxysql/ssl-internal/ca.crt"
        ssl_p2s_cert="/etc/proxysql/ssl-internal/tls.crt"
        ssl_p2s_cipher="ECDHE-RSA-AES128-GCM-SHA256"
        ssl_p2s_key="/etc/proxysql/ssl-internal/tls.key"
        stacksize=1048576
        threads=4
      }
    labels:
      {{ .Values.label_name }}: {{ .Values.label_value }}
      release: {{ .Values.label_value }}
    resources:
      requests:
        memory: 1G
        cpu: 500m
      limits:
        memory: 1G
        cpu: 2000m
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
    volumeSpec:
      #      emptyDir: {}
      #      hostPath:
      #        path: /data
      #        type: Directory
      persistentVolumeClaim:
        storageClassName: {{ .Values.storage_class_name }}
        resources:
          requests:
            storage: 2Gi
    podDisruptionBudget:
      maxUnavailable: 1
    #      minAvailable: 0
    gracePeriod: 30
  pmm:
    enabled: true
    image: percona/percona-xtradb-cluster-operator:1.6.0-pmm
    serverHost: percona-monitoring-server-service
    serverUser: admin
    resources:
      requests:
        memory: 500M
        cpu: 200m
      limits:
        memory: 750M
        cpu: 500m
  backup:
    # There is a breaking bug in the 1.6.0 image for backup agent
    image: percona/percona-xtradb-cluster-operator:1.5.0-pxc8.0-backup
    serviceAccountName: percona-xtradb-cluster-operator
    storages:
      #backup-1:
      #  type: filesystem
      #  labels:
      #    {{ .Values.label_name }}: {{ .Values.label_value }}
      #    release: {{ .Values.label_value }}
      #  volume:
      #    persistentVolumeClaim:
      #      storageClassName: {{ .Values.storage_class_name }}
      #      accessModes: [ "ReadWriteOnce" ]
      #      resources:
      #        requests:
      #          storage: 60Gi

      backup-2:
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_2 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_2 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_2 }}
        region: {{ .Values.s3_region_backup_2 }}

      backup-3:
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_3 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_3 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_3 }}
        region: {{ .Values.s3_region_backup_3 }}

      backup-4:
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_4 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_4 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_4 }}
        region: {{ .Values.s3_region_backup_4 }}

    schedule:
      #- name: backup-1
      #  schedule: '0 * * * *'
      #  keep: 6
      #  storageName: backup-1

      - name: backup-2
        schedule: "0 * * * *"
        keep: 12
        storageName: backup-2

      - name: backup-3
        schedule: "15 * * * *"
        keep: 12
        storageName: backup-3

      - name: backup-4
        schedule: "30 * * * *"
        keep: 12
        storageName: backup-4

