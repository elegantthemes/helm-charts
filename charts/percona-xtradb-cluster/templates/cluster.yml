apiVersion: pxc.percona.com/v1
kind: PerconaXtraDBCluster
metadata:
  name: {{ .Values.cluster_name }}
  labels:
    {{ .Values.label_name }}: {{ .Values.label_value }}
    release: {{ .Values.label_value }}
  finalizers:
    - delete-pxc-pods-in-order
#    - delete-proxysql-pvc
#    - delete-pxc-pvc
#  annotations:
#    percona.com/issue-vault-token: "true"
spec:
  # Set to true to pause the cluster.
  pause: false
  crVersion: 1.12.0
  secretsName: {{ .Values.secrets_name }}
  sslSecretName: {{ .Values.ssl_secrets_name }}
  sslInternalSecretName: {{ .Values.ssl_internal_secrets_name }}
  vaultSecretName: {{ .Values.vault_secrets_name }}
  logCollectorSecretName: {{ .Values.secrets_name }}-log-collector
  allowUnsafeConfigurations: false
  updateStrategy: RollingUpdate
  upgradeOptions:
  # versionServiceEndpoint: https://check.percona.com
    apply: Never
  # schedule: "0 4 * * *"
  pxc:
    size: 3
    # original image has bugged socat that prevents backup from completing.
    # also we need to set et_backup_running WP option during backups so the cloud server can disable activations.
    image: elegantthemes/percona-xtradb-cluster:8.0.29-21.1-wp-option
    autoRecovery: true
#   schedulerName: mycustom-scheduler
#   readinessDelaySec: 15
#   livenessDelaySec: 600
#   forceUnsafeBootstrap: false
    configuration: |
      [mysqld]
      #gtid_mode = ON
      #enforce_gtid_consistency = ON
      #session_track_gtids = OWN_GTID
      wsrep_provider_options = "gcache.size=1G; gcache.recover=yes"
      wsrep_sync_wait = 7
      binlog_expire_logs_seconds=259200
      [sst]
      xbstream-opts = --decompress
      [xtrabackup]
      compress = lz4
      use-memory = 16G
    resources:
      requests:
        memory: 2G
        cpu: 1000m
      limits:
        memory: 32G
        cpu: 6000m
#        ephemeral-storage: 1G
#    nodeSelector:
#      disktype: ssd
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
    podDisruptionBudget:
      maxUnavailable: 1
    volumeSpec:
      persistentVolumeClaim:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 200Gi
        storageClassName: openebs-hostpath
      # hostPath:
      #   path: /data
      #   type: DirectoryOrCreate
      #persistentVolumeClaim:
      #  storageClassName: {{ .Values.storage_class_name }}
      #  accessModes: [ "ReadWriteOnce" ]
      #  resources:
      #    requests:
      #      storage: 6Gi
    gracePeriod: 600
    containerSecurityContext:
      privileged: true
    tolerations:
      - key: node.kubernetes.io/disk-pressure
        operator: Exists
      - key: node.kubernetes.io/memory-pressure
        operator: Exists
      - key: node.kubernetes.io/pid-pressure
        operator: Exists
  haproxy:
    enabled: false
    size: 3
    image: percona/percona-xtradb-cluster-operator:1.10.0-haproxy
#    replicasServiceEnabled: false
#    imagePullPolicy: Always
#    schedulerName: mycustom-scheduler
#    configuration: |
#
#    the actual default configuration file can be found here https://github.com/percona/percona-docker/blob/main/haproxy/dockerdir/etc/haproxy/haproxy-global.cfg
#
#      global
#        maxconn 2048
#        external-check
#        insecure-fork-wanted
#        stats socket /etc/haproxy/pxc/haproxy.sock mode 600 expose-fd listeners level admin
#
#      defaults
#        default-server init-addr last,libc,none
#        log global
#        mode tcp
#        retries 10
#        timeout client 28800s
#        timeout connect 100500
#        timeout server 28800s
#
#      frontend galera-in
#        bind *:3309 accept-proxy
#        bind *:3306
#        mode tcp
#        option clitcpka
#        default_backend galera-nodes
#
#      frontend galera-admin-in
#        bind *:33062
#        mode tcp
#        option clitcpka
#        default_backend galera-admin-nodes
#
#      frontend galera-replica-in
#        bind *:3307
#        mode tcp
#        option clitcpka
#        default_backend galera-replica-nodes
#
#      frontend galera-mysqlx-in
#        bind *:33060
#        mode tcp
#        option clitcpka
#        default_backend galera-mysqlx-nodes
#
#      frontend stats
#        bind *:8404
#        mode http
#        option http-use-htx
#        http-request use-service prometheus-exporter if { path /metrics }
#    imagePullSecrets:
#      - name: private-registry-credentials
#    annotations:
#      iam.amazonaws.com/role: role-arn
#    labels:
#      rack: rack-22
#    readinessProbes:
#      initialDelaySeconds: 15
#      timeoutSeconds: 1
#      periodSeconds: 5
#      successThreshold: 1
#      failureThreshold: 3
#    livenessProbes:
#      initialDelaySeconds: 60
#      timeoutSeconds: 5
#      periodSeconds: 30
#      successThreshold: 1
#      failureThreshold: 4
#    serviceType: ClusterIP
#    externalTrafficPolicy: Cluster
#    replicasServiceType: ClusterIP
#    replicasExternalTrafficPolicy: Cluster
#    runtimeClassName: image-rc
#    sidecars:
#    - image: busybox
#      command: ["/bin/sh"]
#      args: ["-c", "while true; do trap 'exit 0' SIGINT SIGTERM SIGQUIT SIGKILL; done;"]
#      name: my-sidecar-1
#    envVarsSecret: my-env-var-secrets
    resources:
      requests:
        memory: 1G
        cpu: 600m
#      limits:
#        memory: 1G
#        cpu: 700m
#    priorityClassName: high-priority
#    nodeSelector:
#      disktype: ssd
#    sidecarResources:
#      requests:
#        memory: 1G
#        cpu: 500m
#      limits:
#        memory: 2G
#        cpu: 600m
#    serviceAccountName: percona-xtradb-cluster-operator-workload
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
#      advanced:
#        nodeAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#            nodeSelectorTerms:
#            - matchExpressions:
#              - key: kubernetes.io/e2e-az-name
#                operator: In
#                values:
#                - e2e-az1
#                - e2e-az2
#    tolerations:
#    - key: "node.alpha.kubernetes.io/unreachable"
#      operator: "Exists"
#      effect: "NoExecute"
#      tolerationSeconds: 6000
    podDisruptionBudget:
      maxUnavailable: 1
#      minAvailable: 0
    gracePeriod: 30
#   loadBalancerSourceRanges:
#     - 10.0.0.0/8
#   serviceAnnotations:
#     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  proxysql:
    enabled: false
    size: 3
    image: percona/percona-xtradb-cluster-operator:1.10.0-proxysql
#    imagePullPolicy: Always
    configuration: |
      datadir="/var/lib/proxysql"
      errorlog="/var/lib/proxysql/proxysql.log"
    
      admin_variables =
      {
        admin_credentials="proxyadmin:{{ .Values.proxysql_admin_password }}"
        mysql_ifaces="0.0.0.0:6032"
        refresh_interval=2000
    
        cluster_check_interval_ms=200
        cluster_check_status_frequency=100
        cluster_mysql_query_rules_diffs_before_sync=3
        cluster_mysql_query_rules_save_to_disk=true
        cluster_mysql_servers_diffs_before_sync=3
        cluster_mysql_servers_save_to_disk=true
        cluster_mysql_users_diffs_before_sync=3
        cluster_mysql_users_save_to_disk=true
        cluster_password="{{ .Values.proxysql_admin_password }}"
        cluster_proxysql_servers_diffs_before_sync=3
        cluster_proxysql_servers_save_to_disk=true
        cluster_username="proxyadmin"
      }
    
      mysql_variables=
      {
        commands_stats=true
        connect_timeout_server=10000
        default_query_delay=0
        default_query_timeout=10000
        default_schema="information_schema"
        have_ssl=true
        interfaces="0.0.0.0:3306;0.0.0.0:33062"
        max_connections=2048
        monitor_connect_interval=20000
        monitor_galera_healthcheck_interval=5000
        monitor_galera_healthcheck_max_timeout_count=5
        monitor_galera_healthcheck_timeout=1200
        monitor_history=60000
        monitor_password="{{ .Values.proxysql_monitor_password }}"
        monitor_ping_interval=10000
        ping_timeout_server=200
        poll_timeout=2000
        sessions_sort=true
        ssl_p2s_ca="/etc/proxysql/ssl-internal/ca.crt"
        ssl_p2s_cert="/etc/proxysql/ssl-internal/tls.crt"
        ssl_p2s_cipher="ECDHE-RSA-AES128-GCM-SHA256"
        ssl_p2s_key="/etc/proxysql/ssl-internal/tls.key"
        stacksize=1048576
        threads=4
      }
    labels:
      {{ .Values.label_name }}: {{ .Values.label_value }}
      release: {{ .Values.label_value }}
    resources:
      requests:
        memory: 1G
        cpu: 500m
      limits:
        memory: 1G
        cpu: 2000m
    affinity:
      antiAffinityTopologyKey: "kubernetes.io/hostname"
#      advanced:
#        nodeAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#            nodeSelectorTerms:
#            - matchExpressions:
#              - key: kubernetes.io/e2e-az-name
#                operator: In
#                values:
#                - e2e-az1
#                - e2e-az2
#    tolerations:
#    - key: "node.alpha.kubernetes.io/unreachable"
#      operator: "Exists"
#      effect: "NoExecute"
#      tolerationSeconds: 6000
    volumeSpec:
      #      emptyDir: {}
      #      hostPath:
      #        path: /data
      #        type: Directory
      persistentVolumeClaim:
        storageClassName: {{ .Values.storage_class_name }}
        resources:
          requests:
            storage: 2G
    podDisruptionBudget:
      maxUnavailable: 1
    #      minAvailable: 0
    gracePeriod: 30
#   loadBalancerSourceRanges:
#     - 10.0.0.0/8
#   serviceAnnotations:
#     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  logcollector:
    enabled: true
    image: percona/percona-xtradb-cluster-operator:1.12.0-logcollector
#    configuration: |
#      [OUTPUT]
#           Name  es
#           Match *
#           Host  192.168.2.3
#           Port  9200
#           Index my_index
#           Type  my_type
    resources:
      requests:
        memory: 100M
        cpu: 200m
  pmm:
    enabled: false
    image: percona/pmm-client:2.28.0
    serverHost: percona-monitoring-server-service
    serverUser: admin
#    pxcParams: "--disable-tablestats-limit=2000"
#    proxysqlParams: "--custom-labels=CUSTOM-LABELS"
    resources:
      requests:
        memory: 150M
        cpu: 300m
      limits:
        memory: 750M
        cpu: 500m
  backup:
    image: percona/percona-xtradb-cluster-operator:1.12.0-pxc8.0-backup
    # Default backup image uses 10 concurrent S3 connections, since the value is hardcoded, it can only be changed
    # using a custom image: last number in the name specifies the max number of concurrent connections.
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-1
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-2
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-4
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-5
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-6
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-15
    # image: bitfade/percona-xtradb-cluster-operator:1.10.0-pxc8.0.25-backup-20
#    serviceAccountName: percona-xtradb-cluster-operator
#    imagePullSecrets:
#      - name: private-registry-credentials
    pitr:
      enabled: true
      storageName: backup-pitr-1 
      timeBetweenUploads: 120
    storages:
      #backup-1:
      #  type: filesystem
      #  labels:
      #    {{ .Values.label_name }}: {{ .Values.label_value }}
      #    release: {{ .Values.label_value }}
      #  volume:
      #    persistentVolumeClaim:
      #      storageClassName: {{ .Values.storage_class_name }}
      #      accessModes: [ "ReadWriteOnce" ]
      #      resources:
      #        requests:
      #          storage: 60Gi

      backup-aws-1:
        nodeSelector:
          database-node: "true"
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_aws_1 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_aws_1 }}
          region: {{ .Values.s3_region_backup_aws_1 }}

      backup-2:
        nodeSelector:
          database-node: "true"
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_2 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_2 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_2 }}
          region: {{ .Values.s3_region_backup_2 }}

      backup-3:
        nodeSelector:
          database-node: "true"
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_3 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_3 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_3 }}
          region: {{ .Values.s3_region_backup_3 }}

      backup-4:
        nodeSelector:
          database-node: "true"
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_4 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_4 }}
          endpointUrl: {{ .Values.s3_endpoint_backup_4 }}
          region: {{ .Values.s3_region_backup_4 }}

      backup-pitr-1:
        nodeSelector:
          database-node: "true"
        type: s3
        labels:
          {{ .Values.label_name }}: {{ .Values.label_value }}
          release: {{ .Values.label_value }}
        s3:
          bucket: {{ .Values.s3_bucket_name_backup_pitr_1 }}
          credentialsSecret: {{ .Values.s3_secret_name_backup_pitr_1 }}
          region: {{ .Values.s3_region_backup_pitr_1 }}

      backup-local:
        nodeSelector:
          database-node: "true"
        type: filesystem
        volume:
          persistentVolumeClaim:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 10Gi
            storageClassName: {{ .Values.storage_class_name }}

    schedule:
      # At minute 1 past every 12th hour.
      - name: local
        schedule: '1 */12 * * *'
        keep: 14
        storageName: backup-local

      # At minute 1 past every 12th hour from 6 through 23.
      - name: aws
        schedule: "1 6/12 * * *"
        keep: 60
        storageName: backup-aws-1
